{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Business Problem  \nMan cannot live on Seamless alone but after a long day at work they may want to try.  Going to the internet for a recipe idea can be overwhelming, especially if you aren't a practiced cook. I hope to make cooking an easier option for people to choose by creating a model that will, when given a recipe, determine if it is easy to prepare or not.  This can be used by both home chefs and by websites that host recipes, enabling the former to know what they're getting into when they decide to make dinner and providing the latter with the means to auto-classify their catalogs and new submissions. \n\nSomething about how this makes their recipes more accessible, potentially attractive to new users. Focus on the business side not the home users","metadata":{}},{"cell_type":"markdown","source":"## Data Understanding  \n\nThe data used to build my model comes from KAGGLE LINK and consists of ~500,000 user-submitted recipes scraped from Food.com.  In addition to the text of the recipes' description and instructions the dataset also contains columns breaking out the ingredients, search terms, tags, and individual steps for each recipe. The tag data comes from the recipe author from a list of options provided by Food.com whereas the search terms are, from all evidence, assigned by Food.com. There is also an \"id\" column that can be used to search for the recipe on Food.com.\n\nThis dataset does not have a target variable included so one needs to be constructed for it by leveraging the tag and search term data to find dinner recipes that can be called easy, be it because they're quick, simple to make, or have very few steps. ","metadata":{}},{"cell_type":"markdown","source":"## Preliminary Data Cleaning   \n\nThe primary challenge in cleaning the dataset lay in the fact that a number of columns have string data that appears to be in list format:  \n- \"['apple', 'orange']\"  \n    vs.\n- ['apple', 'orange'] \n\nHaving this information in actual list format was a priority as it greatly simplified the EDA phase. \n\nThis being said the overall dataset required very little cleaning - the information in each column was formated consistently and there were very few null values or other missing data. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\n\nfrom wordcloud import WordCloud\nimport ast # used for converting column values to lists post-import from csv\n\nfrom nltk import FreqDist","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:12:23.843863Z","iopub.execute_input":"2021-12-01T03:12:23.844084Z","iopub.status.idle":"2021-12-01T03:12:25.600437Z","shell.execute_reply.started":"2021-12-01T03:12:23.844057Z","shell.execute_reply":"2021-12-01T03:12:25.599524Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/foodcom-recipes-with-search-terms-and-tags/recipes_w_search_terms.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:12:25.602044Z","iopub.execute_input":"2021-12-01T03:12:25.602264Z","iopub.status.idle":"2021-12-01T03:12:47.241601Z","shell.execute_reply.started":"2021-12-01T03:12:25.602236Z","shell.execute_reply":"2021-12-01T03:12:47.240647Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:12:47.242735Z","iopub.execute_input":"2021-12-01T03:12:47.242957Z","iopub.status.idle":"2021-12-01T03:12:47.778354Z","shell.execute_reply.started":"2021-12-01T03:12:47.242930Z","shell.execute_reply":"2021-12-01T03:12:47.777450Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"I chose to simply remove the null values in the \"description\" column; while this removes ~9,600 rows, or just under 2%, of data the overall scale of the dataset can support the loss.","metadata":{}},{"cell_type":"code","source":"# confirm number of null values in 'description'\ndf['description'].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:12:47.779778Z","iopub.execute_input":"2021-12-01T03:12:47.780663Z","iopub.status.idle":"2021-12-01T03:12:47.875981Z","shell.execute_reply.started":"2021-12-01T03:12:47.780615Z","shell.execute_reply":"2021-12-01T03:12:47.874443Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# drop columns with nulls\ndf = df.dropna(subset=['description'])","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:12:47.877258Z","iopub.execute_input":"2021-12-01T03:12:47.878355Z","iopub.status.idle":"2021-12-01T03:12:48.104291Z","shell.execute_reply.started":"2021-12-01T03:12:47.878303Z","shell.execute_reply":"2021-12-01T03:12:48.103338Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df['description'].value_counts()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:12:48.105791Z","iopub.execute_input":"2021-12-01T03:12:48.106028Z","iopub.status.idle":"2021-12-01T03:12:48.763543Z","shell.execute_reply.started":"2021-12-01T03:12:48.105996Z","shell.execute_reply":"2021-12-01T03:12:48.762716Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# recipes using '.' as the descriiption appear to be unique and so I will not be dropping these\n# from the model \ndf_period = df[df['description'] =='.']\ndf_period.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:12:48.764856Z","iopub.execute_input":"2021-12-01T03:12:48.765083Z","iopub.status.idle":"2021-12-01T03:12:48.864654Z","shell.execute_reply.started":"2021-12-01T03:12:48.765054Z","shell.execute_reply":"2021-12-01T03:12:48.863857Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# confirming nulls removed \ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:12:48.866061Z","iopub.execute_input":"2021-12-01T03:12:48.866268Z","iopub.status.idle":"2021-12-01T03:12:49.374688Z","shell.execute_reply.started":"2021-12-01T03:12:48.866242Z","shell.execute_reply":"2021-12-01T03:12:49.373813Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# reviewing other columns to see what other steps need to be taken\ndf.describe(include = 'object')\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:12:49.376349Z","iopub.execute_input":"2021-12-01T03:12:49.376713Z","iopub.status.idle":"2021-12-01T03:12:53.718454Z","shell.execute_reply.started":"2021-12-01T03:12:49.376652Z","shell.execute_reply":"2021-12-01T03:12:53.717582Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Reviewing the above several things stand out, such as the fact that Banana Bread is a very popular recipe, or that there are at least 63 recipes whose ingredients consist of 'paper' and 'cloth'.  Also noteworthy is that at least 32 recipes seem to be duplicates based on the 'steps' output.  Since neither paper or cloth are edible I removed those from the dataset and further investigated the 'steps' column to see if the recipes truly were duplicates - \"Blend all ingredients until smooth\" could apply to many different smoothie or milkshake recipies after all.","metadata":{}},{"cell_type":"code","source":"df_inedible = df[df['ingredients'] == \"['paper', 'cloth']\"]\ndf_inedible.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:12:53.719821Z","iopub.execute_input":"2021-12-01T03:12:53.720130Z","iopub.status.idle":"2021-12-01T03:12:53.814697Z","shell.execute_reply.started":"2021-12-01T03:12:53.720087Z","shell.execute_reply":"2021-12-01T03:12:53.813840Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"A number of napkin folding 'recipes' have been included in the dataset, hence the ingredients 'paper' and 'cloth'.  THese will be removed but it's worth noting that other crafts or non-edible recipes may have snuck through and something to keep an eye out for while working with the data.","metadata":{}},{"cell_type":"code","source":"# removing napkin folding instructions from the dataframe\n\ndf = df[df['ingredients'] != \"['paper', 'cloth']\"]","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:12:53.818523Z","iopub.execute_input":"2021-12-01T03:12:53.818788Z","iopub.status.idle":"2021-12-01T03:12:54.038542Z","shell.execute_reply.started":"2021-12-01T03:12:53.818757Z","shell.execute_reply":"2021-12-01T03:12:54.037813Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# checking the dataframe again to see how the removal of the craft instructions has changed \n# its makeup\ndf.describe(include = 'object')\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:12:54.039633Z","iopub.execute_input":"2021-12-01T03:12:54.039867Z","iopub.status.idle":"2021-12-01T03:12:57.847864Z","shell.execute_reply.started":"2021-12-01T03:12:54.039839Z","shell.execute_reply":"2021-12-01T03:12:57.847231Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_no_ingredients = df[df['ingredients'] == \"[]\"]\ndf_no_ingredients.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:12:57.848949Z","iopub.execute_input":"2021-12-01T03:12:57.849562Z","iopub.status.idle":"2021-12-01T03:12:57.937989Z","shell.execute_reply.started":"2021-12-01T03:12:57.849526Z","shell.execute_reply":"2021-12-01T03:12:57.936988Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# checking the steps of a specific recipe to see if the ingredients are listed within that column\ndf_no_ingredients['steps'].iloc[0]","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:12:57.939654Z","iopub.execute_input":"2021-12-01T03:12:57.940358Z","iopub.status.idle":"2021-12-01T03:12:57.947114Z","shell.execute_reply.started":"2021-12-01T03:12:57.940310Z","shell.execute_reply":"2021-12-01T03:12:57.946293Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# since there are only 20 recipes that do not have their ingredients broken out these will\n# be dropped from the dataframe\ndf = df[df['ingredients'] != \"[]\"]\ndf.describe(include = 'object')\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:12:57.948574Z","iopub.execute_input":"2021-12-01T03:12:57.949178Z","iopub.status.idle":"2021-12-01T03:13:01.816567Z","shell.execute_reply.started":"2021-12-01T03:12:57.949125Z","shell.execute_reply":"2021-12-01T03:13:01.815724Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df_steps = df['steps'].value_counts().to_frame()\ndf_steps.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:13:01.818021Z","iopub.execute_input":"2021-12-01T03:13:01.818540Z","iopub.status.idle":"2021-12-01T03:13:02.324826Z","shell.execute_reply.started":"2021-12-01T03:13:01.818493Z","shell.execute_reply":"2021-12-01T03:13:02.323967Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"df_example = df[df['steps'].str.contains(\"In a large bowl combine flour, yeast and salt. Add 1 5\")] \ndf_example.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:13:02.326262Z","iopub.execute_input":"2021-12-01T03:13:02.326614Z","iopub.status.idle":"2021-12-01T03:13:02.866308Z","shell.execute_reply.started":"2021-12-01T03:13:02.326569Z","shell.execute_reply":"2021-12-01T03:13:02.865729Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Reading the descriptions of one of the duplicate recipes, a \"No-Knead Bread\", it looks like at least some of the duplicates come from Food.com users uplaoding recipes from other sources - in this case the New York Times.  ","metadata":{}},{"cell_type":"code","source":"df_example = df[df['steps'] ==\"['Blend all ingredients until smooth.']\"] \ndf_example.head()\n\t","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:13:02.867381Z","iopub.execute_input":"2021-12-01T03:13:02.867732Z","iopub.status.idle":"2021-12-01T03:13:02.974057Z","shell.execute_reply.started":"2021-12-01T03:13:02.867702Z","shell.execute_reply":"2021-12-01T03:13:02.972868Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# you can see that other recipes with identical steps are for unique dishes, below we see \n# all the options for recipes whose steps are '['Blend all ingredients until smooth.']', \n# the most common string in 'steps'\n\ndf_example['name'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:13:02.975266Z","iopub.execute_input":"2021-12-01T03:13:02.975700Z","iopub.status.idle":"2021-12-01T03:13:02.992527Z","shell.execute_reply.started":"2021-12-01T03:13:02.975654Z","shell.execute_reply":"2021-12-01T03:13:02.991723Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Using the df_steps dataframe to find out how many recipes in total have duplicate steps. \n\ndf_steps_dupe = df_steps[df_steps['steps'] > 1]\ndf_steps_dupe['steps'].sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:13:02.994160Z","iopub.execute_input":"2021-12-01T03:13:02.995110Z","iopub.status.idle":"2021-12-01T03:13:03.010211Z","shell.execute_reply.started":"2021-12-01T03:13:02.994999Z","shell.execute_reply":"2021-12-01T03:13:03.009557Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"2,944 recipes have duplicate steps. Given that we've established this is a mixture of true duplicates, in the cases of recipes copied from other sources, and legitimately unique recipes these will be deleted as the number of recipes affected does not justify the work necessary to further evaluate each's status (although if I were to do this it would be based on steps length to begin with)","metadata":{}},{"cell_type":"code","source":"# removing potential duplicate recipes. Code thanks to first answer on \n# https://stackoverflow.com/questions/49735683/python-removing-rows-on-count-condition\ndf = df[df.groupby('steps').steps.transform('count')==1].copy() \n# add copy for future warning when you need to modify the sub df\nlen(df)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:13:03.011207Z","iopub.execute_input":"2021-12-01T03:13:03.012078Z","iopub.status.idle":"2021-12-01T03:13:05.823940Z","shell.execute_reply.started":"2021-12-01T03:13:03.012030Z","shell.execute_reply":"2021-12-01T03:13:05.823092Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"df.describe(include = 'object')\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:13:05.825534Z","iopub.execute_input":"2021-12-01T03:13:05.825904Z","iopub.status.idle":"2021-12-01T03:13:09.438300Z","shell.execute_reply.started":"2021-12-01T03:13:05.825867Z","shell.execute_reply":"2021-12-01T03:13:09.437372Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Converting Column Data to Lists  \n\nThe two code blocks below highlight the primary issue with the data: the strings that look like lists. This was resolved by creating a custom function that uses the ast library to take in a string with list-like formating and returns an actual list. ","metadata":{}},{"cell_type":"code","source":"# looking at an example from  the 'ingredients' column and comparing it against its type\ndf['ingredients'].iloc[0]","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:13:09.440004Z","iopub.execute_input":"2021-12-01T03:13:09.440335Z","iopub.status.idle":"2021-12-01T03:13:09.447079Z","shell.execute_reply.started":"2021-12-01T03:13:09.440293Z","shell.execute_reply":"2021-12-01T03:13:09.446299Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# although the above output looks like a list checking the type confirms it is a string\ntype(df['ingredients'].iloc[0]) ","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:13:09.448146Z","iopub.execute_input":"2021-12-01T03:13:09.448371Z","iopub.status.idle":"2021-12-01T03:13:09.461272Z","shell.execute_reply.started":"2021-12-01T03:13:09.448342Z","shell.execute_reply":"2021-12-01T03:13:09.460325Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"In order to avoid mistakenly overwriting data or having to concatenate the function's output with the original dataframe return_to_list creates a new column in the dataframe with the converted strings. ","metadata":{}},{"cell_type":"code","source":"\ndef return_to_list(df, column_names):\n    ''' Takes in list of names of columns containing strings and the dataframe they sit in and returns converts each column's contents into a new\n    column, called '<original column name>_list', now as lists. May only work on strings that look like lists.... \n    \n    Inputs:\n    df = dataframe with columns being converted to lists\n    column_names = list of columns whose contents need to be transformed\n    \n    Returns: updated dataframe\n    '''\n    for col in column_names:\n        col_name = col + '_list'\n        df[col_name] = [ast.literal_eval(x) for x in df[col] ]\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:13:09.462360Z","iopub.execute_input":"2021-12-01T03:13:09.462810Z","iopub.status.idle":"2021-12-01T03:13:09.470824Z","shell.execute_reply.started":"2021-12-01T03:13:09.462775Z","shell.execute_reply":"2021-12-01T03:13:09.470082Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# list of list-appearing columns\ncol_to_list = ['ingredients', 'ingredients_raw_str', 'steps', 'tags', 'search_terms']","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:13:09.472469Z","iopub.execute_input":"2021-12-01T03:13:09.473289Z","iopub.status.idle":"2021-12-01T03:13:09.481776Z","shell.execute_reply.started":"2021-12-01T03:13:09.473253Z","shell.execute_reply":"2021-12-01T03:13:09.481145Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# running dataframe through custom function \ndf = return_to_list(df, col_to_list)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:13:09.483220Z","iopub.execute_input":"2021-12-01T03:13:09.483445Z","iopub.status.idle":"2021-12-01T03:14:19.208569Z","shell.execute_reply.started":"2021-12-01T03:13:09.483418Z","shell.execute_reply":"2021-12-01T03:14:19.207709Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# confirming new columns have been created\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:14:19.209845Z","iopub.execute_input":"2021-12-01T03:14:19.210087Z","iopub.status.idle":"2021-12-01T03:14:19.238691Z","shell.execute_reply.started":"2021-12-01T03:14:19.210056Z","shell.execute_reply":"2021-12-01T03:14:19.238090Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# confirming type of data in new columns\ntype(df['ingredients_list'].iloc[0]) ","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:14:19.239719Z","iopub.execute_input":"2021-12-01T03:14:19.240052Z","iopub.status.idle":"2021-12-01T03:14:19.254603Z","shell.execute_reply.started":"2021-12-01T03:14:19.240022Z","shell.execute_reply":"2021-12-01T03:14:19.253595Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# one addtional step needs to be taken - becasue the data in the \"search_terms\" column \n# was enclosed in curly brackets their contents were transformed into a set.  \ntype(df['search_terms_list'].iloc[0]) ","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:14:19.256173Z","iopub.execute_input":"2021-12-01T03:14:19.257196Z","iopub.status.idle":"2021-12-01T03:14:19.269788Z","shell.execute_reply.started":"2021-12-01T03:14:19.257147Z","shell.execute_reply":"2021-12-01T03:14:19.268773Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# For the sake of uniformity \"search_terms_list\" is converted into a list\ndf['search_terms_list'] = df['search_terms_list'].apply(lambda x: list(x))\ntype(df['search_terms_list'].iloc[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:14:19.271509Z","iopub.execute_input":"2021-12-01T03:14:19.272462Z","iopub.status.idle":"2021-12-01T03:14:19.703320Z","shell.execute_reply.started":"2021-12-01T03:14:19.272416Z","shell.execute_reply":"2021-12-01T03:14:19.702439Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Setting the Target Variable \n\nData from two different columns was used to develope the target variable.  The model seeks to find easy dinner recipes and the target included recipes that listed 'dinner' as one of their search terms.  The list of tags was reviewed to find those that fit our target recipe and marking all recipes that contain at least one of the \"easy indicator\" tags.  A recipe had to have at least one of the relevant tags and have 'dinner' as a search term to qualify as a target. \n\nA custom function, lists_to_count, was created to facilitate reviewing the data as well as visualizing it. \n","metadata":{}},{"cell_type":"code","source":"def lists_to_count(df, column, series = False):\n    ''' takes in a column of lists and returns counts for all unique values. \n    \n    Inputs:\n    df - dataframe with column being converted\n    column - column of lists\n    series - if set to True returns pandas Series instead of a FreqDist object \n    \n    Returns: \n    Series with unique value counts or FreqDist object, depending on setting of 'series' parameter\n    '''\n\n    all_col = df[column].explode()\n    col_count = FreqDist(all_col)\n    \n    if series:\n        return pd.Series(dict(col_count))\n    else:\n        return col_count","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:14:19.705085Z","iopub.execute_input":"2021-12-01T03:14:19.705694Z","iopub.status.idle":"2021-12-01T03:14:19.712817Z","shell.execute_reply.started":"2021-12-01T03:14:19.705632Z","shell.execute_reply":"2021-12-01T03:14:19.711974Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### Tag Data Targets","metadata":{}},{"cell_type":"code","source":"# find the count of each unique tag in the dataset\n\ntag_dist = lists_to_count(df, 'tags_list')\ntype(tag_dist)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:14:19.714578Z","iopub.execute_input":"2021-12-01T03:14:19.715241Z","iopub.status.idle":"2021-12-01T03:14:27.017292Z","shell.execute_reply.started":"2021-12-01T03:14:19.715195Z","shell.execute_reply":"2021-12-01T03:14:27.016731Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# number of unique tabs\nlen(tag_dist)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:14:27.018497Z","iopub.execute_input":"2021-12-01T03:14:27.018887Z","iopub.status.idle":"2021-12-01T03:14:27.024006Z","shell.execute_reply.started":"2021-12-01T03:14:27.018835Z","shell.execute_reply":"2021-12-01T03:14:27.023127Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# the below code displays the counts, ordered from most to least frequent, for each tag.\ntag_dist.items()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:14:27.025049Z","iopub.execute_input":"2021-12-01T03:14:27.025260Z","iopub.status.idle":"2021-12-01T03:14:27.039833Z","shell.execute_reply.started":"2021-12-01T03:14:27.025232Z","shell.execute_reply":"2021-12-01T03:14:27.038229Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black').generate_from_frequencies(dict(tag_dist))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:14:27.042100Z","iopub.execute_input":"2021-12-01T03:14:27.042712Z","iopub.status.idle":"2021-12-01T03:14:43.076696Z","shell.execute_reply.started":"2021-12-01T03:14:27.042646Z","shell.execute_reply":"2021-12-01T03:14:43.074975Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"\n#function for visualizing the most common tokens within a frequency distribution\n\ndef visualize_tokens(freq_dist, number, title):\n    '''\n    From Phase 4 Project: \n    https://github.com/CGPinDC/Tweet_NLP_Project/blob/main/Tweet_Sentiment_%20Analysis_Notebook.ipynb\n    \n    Inputs:\n    freq_dist: pass in frequency dictionary of tokens. \n    number: number as integer of the top tokens to return\n    \n    title: title of graph\n    '''\n    \n\n    # get tokens and frequency counts from freq_dist\n    top = list(zip(*freq_dist.most_common(number)))\n    tokens = top[0]\n    counts = top[1]\n    \n    print(f'Top Tokens: {tokens[:number]}')\n\n    # Set up plot and plot data\n    fig, ax = plt.subplots(figsize = (15, 10))\n    ax.bar(tokens, counts)\n\n    # Customize plot appearance\n    ax.set_title(title)\n    ax.set_ylabel(\"Count\")\n    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n    ax.tick_params(axis=\"x\", rotation=90)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:14:43.078184Z","iopub.execute_input":"2021-12-01T03:14:43.078427Z","iopub.status.idle":"2021-12-01T03:14:43.085552Z","shell.execute_reply.started":"2021-12-01T03:14:43.078395Z","shell.execute_reply":"2021-12-01T03:14:43.084725Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"visualize_tokens(tag_dist, 50, \"Top 50 Tags\")","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:14:43.087310Z","iopub.execute_input":"2021-12-01T03:14:43.088041Z","iopub.status.idle":"2021-12-01T03:14:43.889069Z","shell.execute_reply.started":"2021-12-01T03:14:43.087990Z","shell.execute_reply":"2021-12-01T03:14:43.888028Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"Since there are only 631 distinct tags I could review them one-by-one to select the ones that best fit into the \"easy\" category.","metadata":{}},{"cell_type":"code","source":"# prints each tag in an easy to read fashion\nfor tag in list(tag_dist):\n    print(tag)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:14:43.896462Z","iopub.execute_input":"2021-12-01T03:14:43.896721Z","iopub.status.idle":"2021-12-01T03:14:43.997270Z","shell.execute_reply.started":"2021-12-01T03:14:43.896691Z","shell.execute_reply":"2021-12-01T03:14:43.996299Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"Recipes tagged with the following will be included in the target:\n\n* easy     \n* 30-minutes-or-less  \n* 3-steps-or-less  \n* 15-minutes-or-less                        \n* beginner-cook                                     \n\nThe following tags were also discovered during the review.  While consisting of a small number of recipes overall since they are not for food recipes containing these tags will be removed, as we did with the napkin folding instructions. \n\n* bath-beauty                                            \n* household-cleansers               \n* homeopathy-remedies  ","metadata":{}},{"cell_type":"code","source":"target_tags = ['easy','30-minutes-or-less', '3-steps-or-less', \n               '15-minutes-or-less', 'beginner-cook']\ntags_to_remove = ['bath-beauty', 'household-cleaners', 'homeopathy-remedies']","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:14:43.998498Z","iopub.execute_input":"2021-12-01T03:14:43.998732Z","iopub.status.idle":"2021-12-01T03:14:44.003308Z","shell.execute_reply.started":"2021-12-01T03:14:43.998695Z","shell.execute_reply":"2021-12-01T03:14:44.002434Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# we'll remove the homeopathic and bath/beauty related recipes by indicating which\n# recipes contain the undesirable tags\ndf['remove'] = df['tags_list'].map(lambda x: any(tag in x for tag in tags_to_remove))\ndf['remove'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:16.630641Z","iopub.execute_input":"2021-12-01T03:15:16.631480Z","iopub.status.idle":"2021-12-01T03:15:17.435743Z","shell.execute_reply.started":"2021-12-01T03:15:16.631440Z","shell.execute_reply":"2021-12-01T03:15:17.435080Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"df = df[df['remove'] != True]\ndf['remove'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:19.882451Z","iopub.execute_input":"2021-12-01T03:15:19.883570Z","iopub.status.idle":"2021-12-01T03:15:20.471449Z","shell.execute_reply.started":"2021-12-01T03:15:19.883444Z","shell.execute_reply":"2021-12-01T03:15:20.470575Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"### Search Term Targets","metadata":{}},{"cell_type":"code","source":"# begin by generating the count of unique search terms\nsearch_term_dist = lists_to_count(df, 'search_terms_list')\ntype(search_term_dist)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:23.442583Z","iopub.execute_input":"2021-12-01T03:15:23.443253Z","iopub.status.idle":"2021-12-01T03:15:24.742700Z","shell.execute_reply.started":"2021-12-01T03:15:23.443202Z","shell.execute_reply":"2021-12-01T03:15:24.742152Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# Wordcloud visalization of count\n\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black').generate_from_frequencies(dict(search_term_dist))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:24.743998Z","iopub.execute_input":"2021-12-01T03:15:24.744799Z","iopub.status.idle":"2021-12-01T03:15:35.367969Z","shell.execute_reply.started":"2021-12-01T03:15:24.744744Z","shell.execute_reply":"2021-12-01T03:15:35.367057Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# show the most common search terms\nvisualize_tokens(search_term_dist, 50, \"Top 50 Search Terms\")","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:35.369110Z","iopub.execute_input":"2021-12-01T03:15:35.369328Z","iopub.status.idle":"2021-12-01T03:15:36.113153Z","shell.execute_reply.started":"2021-12-01T03:15:35.369301Z","shell.execute_reply":"2021-12-01T03:15:36.112252Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"len(search_term_dist)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:36.115884Z","iopub.execute_input":"2021-12-01T03:15:36.116496Z","iopub.status.idle":"2021-12-01T03:15:36.123825Z","shell.execute_reply.started":"2021-12-01T03:15:36.116447Z","shell.execute_reply":"2021-12-01T03:15:36.122854Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"for term in list(search_term_dist):\n    print(term)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:36.125446Z","iopub.execute_input":"2021-12-01T03:15:36.126441Z","iopub.status.idle":"2021-12-01T03:15:36.150358Z","shell.execute_reply.started":"2021-12-01T03:15:36.126390Z","shell.execute_reply":"2021-12-01T03:15:36.149446Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# generating a count of 'quick'recipes to see if that should be included in the search term target\nsearch_term_dist['quick']","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:36.151818Z","iopub.execute_input":"2021-12-01T03:15:36.152210Z","iopub.status.idle":"2021-12-01T03:15:36.161108Z","shell.execute_reply.started":"2021-12-01T03:15:36.152162Z","shell.execute_reply":"2021-12-01T03:15:36.160059Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"Reviewing the search terms confirms that the only one that's going to be used to create this target variable is \"dinner\".  While there is a \"quick\" search term it applies to only 2,600 recipes, not a material amount. Additionally \"quick\" recipes are not automatically \"dinner\" recipes so their inclusion as a target parameter potentially adds bad data to the target set.","metadata":{}},{"cell_type":"code","source":"# number of recipes labeled with 'dinner' as a search term\nsearch_term_dist['dinner']","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:36.162731Z","iopub.execute_input":"2021-12-01T03:15:36.163528Z","iopub.status.idle":"2021-12-01T03:15:36.171080Z","shell.execute_reply.started":"2021-12-01T03:15:36.163486Z","shell.execute_reply":"2021-12-01T03:15:36.170318Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# creating a list with the search term target label\ntarget_search_term = ['dinner']","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:36.172120Z","iopub.execute_input":"2021-12-01T03:15:36.172391Z","iopub.status.idle":"2021-12-01T03:15:36.180928Z","shell.execute_reply.started":"2021-12-01T03:15:36.172355Z","shell.execute_reply":"2021-12-01T03:15:36.180131Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"### Creating Target Variable","metadata":{}},{"cell_type":"code","source":"# creating a column indicating if a target tag is present in the 'tags_list' column\ndf['target_tag'] = df['tags_list'].map(lambda x: any(tag in x for tag in target_tags))\ndf['target_tag'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:36.182251Z","iopub.execute_input":"2021-12-01T03:15:36.182963Z","iopub.status.idle":"2021-12-01T03:15:36.916686Z","shell.execute_reply.started":"2021-12-01T03:15:36.182918Z","shell.execute_reply":"2021-12-01T03:15:36.915837Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:36.919596Z","iopub.execute_input":"2021-12-01T03:15:36.920010Z","iopub.status.idle":"2021-12-01T03:15:36.967852Z","shell.execute_reply.started":"2021-12-01T03:15:36.919962Z","shell.execute_reply":"2021-12-01T03:15:36.967008Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"Reviewing the above dataframe output I saw the last row contained a recipe with the '60-minutes-or-less' tag but also had one of the target tags.  Seeing how a recipe cold conceivably, if over-optimistically, be tagged with both 'easy' and '60-minutes-or-less', and because this model seeks to find recipes that are both quick and simple, going to reset the \"target_tag\" value to \"False\" if the tag list contains \"60-minutes-or-less\" . ","metadata":{}},{"cell_type":"code","source":"# small function to relabled the 'target_tag' value for recipes with '60-minutes-or-less' \n# tags\ndef hour_check(x):\n    if '60-minutes-or-less' in x: \n        return False\n    else:\n        return True","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:36.969097Z","iopub.execute_input":"2021-12-01T03:15:36.969308Z","iopub.status.idle":"2021-12-01T03:15:36.973544Z","shell.execute_reply.started":"2021-12-01T03:15:36.969282Z","shell.execute_reply":"2021-12-01T03:15:36.972743Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# credit to first response on https://stackoverflow.com/questions/58562662/apply-function-on-subset-of-dataframe-rows-in-column-based-on-value-in-other-col\n# for helping me figure out how to re-label the 'target_tag' column\ndf['target_tag'] = df.apply(lambda row:\n    hour_check(row.tags_list) if row.target_tag == True else row.target_tag, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:36.974830Z","iopub.execute_input":"2021-12-01T03:15:36.975171Z","iopub.status.idle":"2021-12-01T03:15:50.694657Z","shell.execute_reply.started":"2021-12-01T03:15:36.975128Z","shell.execute_reply":"2021-12-01T03:15:50.693743Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"df['target_tag'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:50.696135Z","iopub.execute_input":"2021-12-01T03:15:50.696577Z","iopub.status.idle":"2021-12-01T03:15:50.708103Z","shell.execute_reply.started":"2021-12-01T03:15:50.696531Z","shell.execute_reply":"2021-12-01T03:15:50.707158Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# identifying the recipes with the target search term\ndf['target_search_term'] = df['search_terms_list'].map(lambda x: any(term in x for term in target_search_term))\ndf['target_search_term'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:50.709745Z","iopub.execute_input":"2021-12-01T03:15:50.710257Z","iopub.status.idle":"2021-12-01T03:15:51.160236Z","shell.execute_reply.started":"2021-12-01T03:15:50.710212Z","shell.execute_reply":"2021-12-01T03:15:51.159240Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"Once the recipes were labeled with the target conditions the below code identifies recipes that meet both the search term condition and the tag condition.","metadata":{}},{"cell_type":"code","source":"def target_check(x):\n    ''' dataframe specific function to set rows as meeting the conditions for the \n    target variable or not'''\n    if (x['target_tag'] == True) and (x['target_search_term'] == True):\n        return 1\n    else:\n        return 0","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:51.161336Z","iopub.execute_input":"2021-12-01T03:15:51.161562Z","iopub.status.idle":"2021-12-01T03:15:51.166200Z","shell.execute_reply.started":"2021-12-01T03:15:51.161537Z","shell.execute_reply":"2021-12-01T03:15:51.165434Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"df['target'] = df.apply(target_check, axis=1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:51.168802Z","iopub.execute_input":"2021-12-01T03:15:51.169027Z","iopub.status.idle":"2021-12-01T03:15:59.516404Z","shell.execute_reply.started":"2021-12-01T03:15:51.168998Z","shell.execute_reply":"2021-12-01T03:15:59.515411Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# checking the number of 'easy' recipes in the dataset\ndf['target'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:59.518280Z","iopub.execute_input":"2021-12-01T03:15:59.518574Z","iopub.status.idle":"2021-12-01T03:15:59.530006Z","shell.execute_reply.started":"2021-12-01T03:15:59.518535Z","shell.execute_reply":"2021-12-01T03:15:59.529064Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"Although the distribution of the search term target and the tag target were roughly even with both conditions applied to the recipe we now have an imbalanced dataset, which will have to be addressed prior to modeling.\n\nThe above establishes the modeless baseline for subsequent model performance - needs to correctly pick \"easy\" recipes at least 20% of the time to be better then guessing.","metadata":{}},{"cell_type":"code","source":"# final clean-up action (for now) \ndf.drop(['remove'], axis=1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:59.531762Z","iopub.execute_input":"2021-12-01T03:15:59.532003Z","iopub.status.idle":"2021-12-01T03:15:59.676595Z","shell.execute_reply.started":"2021-12-01T03:15:59.531970Z","shell.execute_reply":"2021-12-01T03:15:59.675931Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:15:59.677731Z","iopub.execute_input":"2021-12-01T03:15:59.678054Z","iopub.status.idle":"2021-12-01T03:16:00.490571Z","shell.execute_reply.started":"2021-12-01T03:15:59.678028Z","shell.execute_reply":"2021-12-01T03:16:00.489952Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"## EDA\n\nNow that the data has had it's initial cleaning (because there's always more you can do on that front) EDA could be performed to further understand the dataset's contents. To begin with we'll create some features that will be used in our first, baseline, model, which will only use length/count data in order to ensure that the target variable can't be identified by these factors alone and to provide a goal when modeling. ","metadata":{}},{"cell_type":"code","source":"import seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:16:00.491935Z","iopub.execute_input":"2021-12-01T03:16:00.492153Z","iopub.status.idle":"2021-12-01T03:16:00.622941Z","shell.execute_reply.started":"2021-12-01T03:16:00.492127Z","shell.execute_reply":"2021-12-01T03:16:00.622160Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# create a number of new columns with counts of list-items and characters. \n\ndf['num_ingredients'] = df['ingredients_list'].apply(lambda x: len(x))\ndf['num_steps'] = df['steps_list'].apply(lambda x: len(x))\ndf['num_char_description'] = df['description'].apply(lambda x: len(x))","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:16:00.623985Z","iopub.execute_input":"2021-12-01T03:16:00.624232Z","iopub.status.idle":"2021-12-01T03:16:01.536316Z","shell.execute_reply.started":"2021-12-01T03:16:00.624203Z","shell.execute_reply":"2021-12-01T03:16:01.535283Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# visualizing our imbalanced target variables\n\ndf.target.value_counts().plot(kind ='bar')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:16:01.537589Z","iopub.execute_input":"2021-12-01T03:16:01.538168Z","iopub.status.idle":"2021-12-01T03:16:01.753563Z","shell.execute_reply.started":"2021-12-01T03:16:01.538131Z","shell.execute_reply":"2021-12-01T03:16:01.752803Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# creating Series of counts to facilitate visualizations\ningredients_count = lists_to_count(df, 'ingredients_list', series = True).sort_values(ascending=False)\nsearch_terms_count = lists_to_count(df, 'search_terms_list', series = True).sort_values(ascending=False)\ntags_count = lists_to_count(df, 'tags_list', series = True).sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:16:01.754844Z","iopub.execute_input":"2021-12-01T03:16:01.755057Z","iopub.status.idle":"2021-12-01T03:16:14.490843Z","shell.execute_reply.started":"2021-12-01T03:16:01.755030Z","shell.execute_reply":"2021-12-01T03:16:14.489877Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(15,7))\nfig.suptitle('20 Most Common Search Terms')\n\nsns.barplot(ax = ax[0], x=search_terms_count.index[:20], y=ingredients_count.values[:20]/ingredients_count.sum())\nax[0].set_title('20 Most Common Search Terms')\nax[0].tick_params(axis = 'x', rotation = 90);\n\nsns.barplot(ax = ax[1], x=search_terms_count.index[:20], y=search_terms_count.values[:20]/search_terms_count.sum())\nax[1].set_title('% of Recipes Listed w/ 20 Most Common Search Terms')\nax[1].tick_params(axis = 'x', rotation = 90);","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:16:14.492053Z","iopub.execute_input":"2021-12-01T03:16:14.492275Z","iopub.status.idle":"2021-12-01T03:16:15.200753Z","shell.execute_reply.started":"2021-12-01T03:16:14.492246Z","shell.execute_reply":"2021-12-01T03:16:15.199875Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,10))\n\n# plotting the 20 most frequently used ingredients \nall_plot = sns.barplot(x=tags_count.index[:20], y=tags_count.values[:20], ax=ax)\nplt.xticks(rotation=90);\nplt.title('20 Most Common Tags')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:16:15.201948Z","iopub.execute_input":"2021-12-01T03:16:15.202163Z","iopub.status.idle":"2021-12-01T03:16:15.608747Z","shell.execute_reply.started":"2021-12-01T03:16:15.202134Z","shell.execute_reply":"2021-12-01T03:16:15.607841Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(15,7))\nfig.suptitle('20 Most Common Ingredients')\n\n# plotting the 20 most frequently used ingredients \nsns.barplot(ax = ax[0],x=ingredients_count.index[:20], y=ingredients_count.values[:20])\nax[0].set_title('20 Most Common Ingredients')\nax[0].tick_params(axis = 'x', rotation = 90);\n\n\nsns.barplot(ax=ax[1], x=ingredients_count.index[:20], y=ingredients_count.values[:20]/ingredients_count.sum())\nax[1].set_title('% Recipies w/the 20 Most Common Ingredients')\nax[1].tick_params(axis = 'x', rotation = 90);\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:16:15.610144Z","iopub.execute_input":"2021-12-01T03:16:15.610366Z","iopub.status.idle":"2021-12-01T03:16:17.505243Z","shell.execute_reply.started":"2021-12-01T03:16:15.610337Z","shell.execute_reply":"2021-12-01T03:16:17.504444Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"The most common ingredients are, as to be expected, cooking essentials: salt, butter, sugar, etc... Decided to investigate this further as we may want to add very common ingredients to our stopwords list prior to vectorization if they are common in both target and non-target recipes","metadata":{}},{"cell_type":"code","source":"# creating ingredient counts for target and non-target recipes\neasy_ingredient_count = lists_to_count(df[df['target'] == 1], 'ingredients_list', series = True).sort_values(ascending=False)\nnot_easy_ingredient_count = lists_to_count(df[df['target'] == 0], 'ingredients_list', series = True).sort_values(ascending=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:16:17.506714Z","iopub.execute_input":"2021-12-01T03:16:17.506985Z","iopub.status.idle":"2021-12-01T03:16:22.079614Z","shell.execute_reply.started":"2021-12-01T03:16:17.506957Z","shell.execute_reply":"2021-12-01T03:16:22.078839Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# plotting the 20 most frequently used ingredients in each type of recipe \n\nfig, ax = plt.subplots(nrows=1, ncols=2,figsize=(15,7))\nfig.suptitle('20 Most Common Ingredients in Target and Non-Target Recipes')\n\nsns.barplot(ax=ax[0], x=easy_ingredient_count.index[:20], y=easy_ingredient_count.values[:20])\nax[0].set_title('Target Recipes')\nax[0].tick_params(axis = 'x', rotation = 90);\n\n\nsns.barplot(ax=ax[1], x=not_easy_ingredient_count.index[:20], y=not_easy_ingredient_count.values[:20])\nax[1].set_title('Non-Target Recipes')\nax[1].tick_params(axis = 'x', rotation = 90);","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:16:22.080871Z","iopub.execute_input":"2021-12-01T03:16:22.081058Z","iopub.status.idle":"2021-12-01T03:16:22.948656Z","shell.execute_reply.started":"2021-12-01T03:16:22.081034Z","shell.execute_reply":"2021-12-01T03:16:22.947693Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"Comparing the two graphs you can see that the Non-Target recipes have more baking related ingredients, such as flour, brown sugar, and baking powder, then the Target recipes do.  This makes sense given that in general baking takes more time then cooking and because the Non-Target column contains recipes with 'dessert' as a search term, which we can see above is the second most common search term after 'dinner'.","metadata":{}},{"cell_type":"markdown","source":"# TO DO\n\nMake the cool graph from the challenge  \nRevise all graph pairings to show Target and Non-Target counts","metadata":{}},{"cell_type":"markdown","source":"### Exploring the 'steps' and 'description' Columns\n\nThe 'steps' and 'description' column text will be the core of the information used in the modeling process and so requires a better understanding of their raw data as well as what emerges after the initial pre-processing steps are taken (converting to lowercase, removing punctuation, etc.). ","metadata":{}},{"cell_type":"code","source":"import re\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:16:22.952116Z","iopub.execute_input":"2021-12-01T03:16:22.952382Z","iopub.status.idle":"2021-12-01T03:16:22.956353Z","shell.execute_reply.started":"2021-12-01T03:16:22.952350Z","shell.execute_reply":"2021-12-01T03:16:22.955403Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"def basic_cleaning(df, column):\n    ''' Takes in a dataframe and the name of the column to be cleaned.  The contents of the column \n    which need to be strings, are converted to lowercase, have their punctuation and numbers removed,\n    and are finally stripped of whitespaces\n    \n    Input:\n    df - dataframe with column to be cleaned\n    column - column containing strings\n    \n    Returns: \n    Dataframe with new, cleaned, column added'''\n    new_col = 'cleaned_' +column\n    # convert to lowercase\n    df[new_col] = df[column].apply(lambda x: x.lower())\n    \n    # remove punctuation and non-characters\n    df[new_col] = df[new_col].apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n    df[new_col] = df[new_col].apply(lambda x: re.sub('[0-9\\n]',' ',x))\n\n    #strip whitespace\n    df[new_col] = df[new_col].apply(lambda x: re.sub('[ ]{2,}',' ',x))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:16:22.957550Z","iopub.execute_input":"2021-12-01T03:16:22.957777Z","iopub.status.idle":"2021-12-01T03:16:22.968455Z","shell.execute_reply.started":"2021-12-01T03:16:22.957749Z","shell.execute_reply":"2021-12-01T03:16:22.967891Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"df = basic_cleaning(df, 'steps')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:16:22.969500Z","iopub.execute_input":"2021-12-01T03:16:22.969931Z","iopub.status.idle":"2021-12-01T03:16:59.583057Z","shell.execute_reply.started":"2021-12-01T03:16:22.969893Z","shell.execute_reply":"2021-12-01T03:16:59.582212Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# review a cleaned step to confirm it appears as expected\nstep = df['cleaned_steps'][0]\nstep","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:16:59.584606Z","iopub.execute_input":"2021-12-01T03:16:59.584847Z","iopub.status.idle":"2021-12-01T03:16:59.601138Z","shell.execute_reply.started":"2021-12-01T03:16:59.584819Z","shell.execute_reply":"2021-12-01T03:16:59.600358Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"One interesting thing to note in the above 'steps' example is that because the recipes are user submitted they may contain spelling errors - in the above case you can see the first word is missing an 'n' -  as they aren't required to meet any proofreading standards. The Spark NLP library contains a spell checker that's used after tokenization and which I may use to resolve this issue. ","metadata":{}},{"cell_type":"code","source":"df['step_tokens'] = df['cleaned_steps'].apply(lambda x: x.split())\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:16:59.602590Z","iopub.execute_input":"2021-12-01T03:16:59.603181Z","iopub.status.idle":"2021-12-01T03:17:11.398382Z","shell.execute_reply.started":"2021-12-01T03:16:59.603138Z","shell.execute_reply":"2021-12-01T03:17:11.397638Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"token_count = lists_to_count(df, 'step_tokens', series = True).sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:17:11.399542Z","iopub.execute_input":"2021-12-01T03:17:11.399852Z","iopub.status.idle":"2021-12-01T03:17:56.029175Z","shell.execute_reply.started":"2021-12-01T03:17:11.399823Z","shell.execute_reply":"2021-12-01T03:17:56.028298Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"#visualization of the most common words in 'steps'\n\nfig, ax = plt.subplots(figsize=(12,4))\n\nsns.barplot(ax=ax, x=token_count.index[:20], y=token_count.values[:20])\nax.set_title('Most Common Words in Steps')\nax.set_xticklabels(token_count.index[:20])\nax.tick_params(axis='x', labelrotation=70);","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:17:56.030417Z","iopub.execute_input":"2021-12-01T03:17:56.030696Z","iopub.status.idle":"2021-12-01T03:17:56.423524Z","shell.execute_reply.started":"2021-12-01T03:17:56.030646Z","shell.execute_reply":"2021-12-01T03:17:56.422626Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"The most common words in 'steps' at this point are, unsurprisingly, stopwords. In order to get a better picture of the data these will be removed and the top 20 words will be replotted. ","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:17:56.424858Z","iopub.execute_input":"2021-12-01T03:17:56.425110Z","iopub.status.idle":"2021-12-01T03:17:56.429235Z","shell.execute_reply.started":"2021-12-01T03:17:56.425079Z","shell.execute_reply":"2021-12-01T03:17:56.428599Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# creating a list of the nltk's English-language stopwords\nstop_words = stopwords.words('english')\nstop_words[:10]","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:17:56.430399Z","iopub.execute_input":"2021-12-01T03:17:56.430873Z","iopub.status.idle":"2021-12-01T03:17:56.450233Z","shell.execute_reply.started":"2021-12-01T03:17:56.430829Z","shell.execute_reply":"2021-12-01T03:17:56.449219Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"# a small function to quickly remove stopwords from the 'step_tokens' column \ndef remove_stop_words(count, stop_words):\n    for x in count.index:\n        if x in stop_words:\n            count = count.drop(x)\n    \n    return count","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:17:56.451421Z","iopub.execute_input":"2021-12-01T03:17:56.451822Z","iopub.status.idle":"2021-12-01T03:17:56.455964Z","shell.execute_reply.started":"2021-12-01T03:17:56.451783Z","shell.execute_reply":"2021-12-01T03:17:56.455367Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"token_count = remove_stop_words(token_count, stop_words)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:17:56.456983Z","iopub.execute_input":"2021-12-01T03:17:56.457293Z","iopub.status.idle":"2021-12-01T03:18:09.009868Z","shell.execute_reply.started":"2021-12-01T03:17:56.457265Z","shell.execute_reply":"2021-12-01T03:18:09.009024Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"# re-plotting the most common words in steps\n\nfig, ax = plt.subplots(figsize=(12,4))\n\nsns.barplot(ax=ax, x=token_count.index[:20], y=token_count.values[:20])\nax.set_title('Most Common Words in Steps, Excluding Stopwords')\nax.set_xticklabels(token_count.index[:20])\nax.tick_params(axis='x', labelrotation=70);","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:09.011272Z","iopub.execute_input":"2021-12-01T03:18:09.011616Z","iopub.status.idle":"2021-12-01T03:18:09.413631Z","shell.execute_reply.started":"2021-12-01T03:18:09.011572Z","shell.execute_reply":"2021-12-01T03:18:09.412741Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"Having reviewed the common contents of 'steps' we'll now do the same for the 'description' column ","metadata":{}},{"cell_type":"code","source":"df = basic_cleaning(df, 'description')\ndf['description_tokens'] = df['cleaned_description'].apply(lambda x: x.split())\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:09.415277Z","iopub.execute_input":"2021-12-01T03:18:09.415584Z","iopub.status.idle":"2021-12-01T03:18:25.512665Z","shell.execute_reply.started":"2021-12-01T03:18:09.415541Z","shell.execute_reply":"2021-12-01T03:18:25.511733Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"description_count = lists_to_count(df, 'description_tokens', series = True).sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:25.513945Z","iopub.execute_input":"2021-12-01T03:18:25.514201Z","iopub.status.idle":"2021-12-01T03:18:42.554628Z","shell.execute_reply.started":"2021-12-01T03:18:25.514169Z","shell.execute_reply":"2021-12-01T03:18:42.553735Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"#visualization of the most common words in 'description'\n\nfig, ax = plt.subplots(figsize=(12,4))\n\nsns.barplot(ax=ax, x=description_count.index[:20], y=description_count.values[:20])\nax.set_title('Most Common Words in Description')\nax.set_xticklabels(description_count.index[:20])\nax.tick_params(axis='x', labelrotation=70);","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:42.556033Z","iopub.execute_input":"2021-12-01T03:18:42.556357Z","iopub.status.idle":"2021-12-01T03:18:42.938369Z","shell.execute_reply.started":"2021-12-01T03:18:42.556312Z","shell.execute_reply":"2021-12-01T03:18:42.937458Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":"Once again we see that the initial visualization is predominantly stopwords. ","metadata":{}},{"cell_type":"code","source":"# remove stopwords from 'description_count'\ndescription_count = remove_stop_words(description_count, stop_words)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:42.939481Z","iopub.execute_input":"2021-12-01T03:18:42.939753Z","iopub.status.idle":"2021-12-01T03:18:54.057220Z","shell.execute_reply.started":"2021-12-01T03:18:42.939716Z","shell.execute_reply":"2021-12-01T03:18:54.056021Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,4))\n\nsns.barplot(ax=ax, x=description_count.index[:20], y=description_count.values[:20])\nax.set_title('Most Common Words in Description, Excluding Stopwords')\nax.set_xticklabels(description_count.index[:20])\nax.tick_params(axis='x', labelrotation=70);","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:54.058518Z","iopub.execute_input":"2021-12-01T03:18:54.058790Z","iopub.status.idle":"2021-12-01T03:18:54.437391Z","shell.execute_reply.started":"2021-12-01T03:18:54.058756Z","shell.execute_reply":"2021-12-01T03:18:54.436554Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"Once stopwords were removed from both 'steps' and 'description' there were no words remaining that seemed to need to be added to the stopwords list. ","metadata":{}},{"cell_type":"markdown","source":"# TO DO  \nDo graphs comparing target/non-target recipes","metadata":{}},{"cell_type":"markdown","source":"## Baseline Model\n\nThis model uses the length data generated earlier to ensure that NLP is an appropriate approach for predicting the difficulty level of a recipe. To begin with we'll create a dataframe that only uses the 'num' columns in the current dataframe.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score, plot_confusion_matrix, plot_roc_curve, accuracy_score, precision_score, recall_score, f1_score\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score, plot_confusion_matrix, plot_roc_curve, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nfrom imblearn.under_sampling import RandomUnderSampler\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:54.438446Z","iopub.execute_input":"2021-12-01T03:18:54.438632Z","iopub.status.idle":"2021-12-01T03:18:54.658718Z","shell.execute_reply.started":"2021-12-01T03:18:54.438608Z","shell.execute_reply":"2021-12-01T03:18:54.657807Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"# Creating a numbers only db\n\nX_numbers_df = df[['num_ingredients','num_steps', 'num_char_description']]\ny_numbers_df = df['target']\nX_numbers_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:54.659930Z","iopub.execute_input":"2021-12-01T03:18:54.660261Z","iopub.status.idle":"2021-12-01T03:18:55.252596Z","shell.execute_reply.started":"2021-12-01T03:18:54.660225Z","shell.execute_reply":"2021-12-01T03:18:55.251975Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"y_numbers_df.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:55.253602Z","iopub.execute_input":"2021-12-01T03:18:55.254224Z","iopub.status.idle":"2021-12-01T03:18:55.264046Z","shell.execute_reply.started":"2021-12-01T03:18:55.254190Z","shell.execute_reply":"2021-12-01T03:18:55.263151Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"The first action that needs to be taken with the 'numbers-only' data is to address the fact that it is unbalanced.  Traditionally the two primary means of handling this are:\n\n- creating more examples of the minority class using SMOTE or something like it\n- removing examples from the majority class so that it has the same number of records as the minority class\n\nBecause the imbalance in this dataset is great enough that using SMOTE would result in there being three times as much synthetic target data as there is real data, and because the dataset is large enough to remove examples that does not match the target and still have almost 200,000 records, undersampling will be used to balance the dataset using Imbalanced Learn's RandomUnderSampler, which under-samples the majority class by randomly picking samples, in this case without replacement. ","metadata":{}},{"cell_type":"code","source":"# please note that this step can be taken prior to the train/test split as no data is \n# transformed, only removed. \nrus = RandomUnderSampler(random_state=50)\nX_res, y_res = rus.fit_resample(X_numbers_df, y_numbers_df)\ny_res.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:55.270617Z","iopub.execute_input":"2021-12-01T03:18:55.271784Z","iopub.status.idle":"2021-12-01T03:18:55.477835Z","shell.execute_reply.started":"2021-12-01T03:18:55.271744Z","shell.execute_reply":"2021-12-01T03:18:55.477010Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"The balanced dataset can now be spit into train and test sets.  Because this is only a baseline model a valedation set is not being created. ","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = .3)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:55.479135Z","iopub.execute_input":"2021-12-01T03:18:55.479349Z","iopub.status.idle":"2021-12-01T03:18:55.502330Z","shell.execute_reply.started":"2021-12-01T03:18:55.479322Z","shell.execute_reply":"2021-12-01T03:18:55.501582Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"y_train.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:55.503652Z","iopub.execute_input":"2021-12-01T03:18:55.503901Z","iopub.status.idle":"2021-12-01T03:18:55.511403Z","shell.execute_reply.started":"2021-12-01T03:18:55.503872Z","shell.execute_reply":"2021-12-01T03:18:55.510557Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"y_test.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:55.512860Z","iopub.execute_input":"2021-12-01T03:18:55.513502Z","iopub.status.idle":"2021-12-01T03:18:55.525666Z","shell.execute_reply.started":"2021-12-01T03:18:55.513453Z","shell.execute_reply":"2021-12-01T03:18:55.524662Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"# creating the StandardScaler object to run the data through so that undue weight isn't given \n# to columns with higher numbers. \n\nscalar = StandardScaler()\nX_train_numbers_scaled = scalar.fit_transform(X_train, y_train)\nX_train_numbers_scaled\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:55.527116Z","iopub.execute_input":"2021-12-01T03:18:55.527854Z","iopub.status.idle":"2021-12-01T03:18:55.548257Z","shell.execute_reply.started":"2021-12-01T03:18:55.527805Z","shell.execute_reply":"2021-12-01T03:18:55.547684Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"# transforming the testing data\nX_test_numbers_scaled = scalar.transform(X_test)\nX_test_numbers_scaled","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:55.549160Z","iopub.execute_input":"2021-12-01T03:18:55.549649Z","iopub.status.idle":"2021-12-01T03:18:55.558075Z","shell.execute_reply.started":"2021-12-01T03:18:55.549620Z","shell.execute_reply":"2021-12-01T03:18:55.557168Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"markdown","source":"We will be using the below custom evaluate function to judge the performance of this, and other, models. ","metadata":{}},{"cell_type":"code","source":"def evaluate(estimator, X_tr, X_te, y_tr, y_te, cv=5, grid_search=False):\n    '''\nFunction takes in estimator, training data, test data, a Boolean value indicating if the estimator is a grid search, \nand the cross validation splitting strategy if the estimator is not a grid search, \nand returns the accuracy, precision, recall, f1, and the ROC-AUC scores for the model \nand a confusion matrix visualization.  From Phase 3 Project: https://github.com/Nindorph/TanzanianWaterWells/blob/main/Modeling_Final.ipynb\n\nIf ‘grid_search parameter is set to “True” then the function will not perform cross validation on the model. \nBased off of Lindsey Berlin’s evaluate function found at: \nhttps://github.com/lindseyberlin/Cat-in-the-Dat-Project/blob/main/notebooks/Lindsey/EDA-Initial-Models.ipynb\n------------------------------------------------------------------------------------------\nInputs: \n-Estimator - Estimator object  \n-X_tr – X_train dataframe\n-X_te – X_test dataframe\n-Y_tr – y_train dataframe\n-Y_te – y_test dataframe\n-Cv – If cross_val  set to true this determines the cross-validation splitting strategy.  \n        Takes in all value options for sklearn.model_selection_cross_val_score “cv” parameter:\n        - None, to use the default 5-fold cross validation,\n        - int, to specify the number of folds in a (Stratified)KFold,\n        - CV splitter,\n        - An iterable yielding (train, test) splits as arrays of indices\n- grid_search – “ Boolean indicating whether a the estimator is a GridSearchCV object, \n        if set to “False” a cross validation will be performed with the number of iterations set by the “cv” parameter.  \n        Default value is “False”.\n\nReturns – nothing is returned \n\n\n    '''\n    #If no grid search is being performed, go through evaluation steps as normal, including cross validation\n    if grid_search == False:\n        #Cross-Validate\n        output = cross_validate(estimator, X_tr, y_tr, cv=cv,\n                                scoring=['accuracy', 'precision','recall', 'f1', 'roc_auc'])\n        #Printing out the mean of all of our evaluating metrics across the cross validation. \n        #Accuracy, precisionc recall, f1, and roc auc\n        print('Results of Cross-Validation:\\n')\n        print(f'Average accuracy: {output[\"test_accuracy\"].mean()}\\\n        +/- {output[\"test_accuracy\"].std()}')\n        print(f'Average precision: {output[\"test_precision\"].mean()}\\\n        +/- {output[\"test_precision\"].std()}')\n        print(f'**Average recall: {output[\"test_recall\"].mean()}\\\n        +/- {output[\"test_recall\"].std()}')\n        print(f'Average f1 score: {output[\"test_f1\"].mean()}\\\n        +/- {output[\"test_f1\"].std()}')\n        print(f'Average roc_auc: {output[\"test_roc_auc\"].mean()}\\\n        +/- {output[\"test_roc_auc\"].std()}\\n')\n        print('+'*20)\n    \n        \n        #Fitting the estimator to our X and y train data\n        estimator.fit(X_tr, y_tr)\n        #getting predictions for X train\n        tr_preds = estimator.predict(X_tr)\n        #getting predictions for X test\n        te_preds = estimator.predict(X_te)\n        \n        #Creating a confusion matrix from our data with custom labels\n        print('\\nResults of Train-Test Split Validation:')\n        plot_confusion_matrix(estimator, X_te, y_te, cmap='mako')\n        \n        #Printing our final evaluating metrics across X train\n        #Evaluating using accuracy, precision, recall, f1, roc auc\n        print(\"\\nTraining Scores:\")\n        print(f\"Train accuracy: {accuracy_score(y_tr, tr_preds)}\")\n        print(f\"Train precision: {precision_score(y_tr, tr_preds)}\")\n        print(f\"**Train recall: {recall_score(y_tr, tr_preds)}\")\n        print(f\"Train f1 score: {f1_score(y_tr, tr_preds)}\")\n        print(f\"Train roc_auc: {roc_auc_score(y_tr, tr_preds)}\\n\")\n        print(\"<>\"*10)\n        #Printing our final evaluating metrics across X test\n        #Evaluating using accuracy, precision, recall, f1, roc auc\n        print(\"\\nTesting Scores:\")\n        print(f\"Test accuracy: {accuracy_score(y_te, te_preds)}\")\n        print(f\"Test precision: {precision_score(y_te, te_preds)}\")\n        print(f\"**Test recall: {recall_score(y_te, te_preds)}\")\n        print(f\"Test f1 score: {f1_score(y_te, te_preds)}\")\n        print(f\"Test roc_auc: {roc_auc_score(y_te, te_preds)}\")\n    \n    #If a grid search is being performed, do not perform a cross validate.\n    else:\n        #Fitting the estimator to our X and y train data\n        estimator.fit(X_tr, y_tr)\n        #getting predictions for X train\n        tr_preds = estimator.predict(X_tr)\n        #getting predictions for X test\n        te_preds = estimator.predict(X_te)\n        \n        #Creating a confusion matrix from our data with custom labels\n        print('\\nResults of Train-Test Split Validation:')\n        plot_confusion_matrix(estimator, X_te, y_te, cmap='mako')\n                              \n        \n        #Printing our final evaluating metrics across X train \n        #Evaluating using accuracy, precision, recall, f1, roc auc\n        print(\"\\nTraining Scores:\")\n        print(f\"Train accuracy: {accuracy_score(y_tr, tr_preds)}\")\n        print(f\"Train precision: {precision_score(y_tr, tr_preds)}\")\n        print(f\"Train recall: {recall_score(y_tr, tr_preds)}\")\n        print(f\"Train f1 score: {f1_score(y_tr, tr_preds)}\")\n        print(f\"Train roc_auc: {roc_auc_score(y_tr, tr_preds)}\\n\")\n        print(\"<>\"*10)\n        \n        #Printing our final evaluating metrics across X test\n        #Evaluating using accuracy, precision, recall, f1, roc auc\n        print(\"\\nTesting Scores:\")\n        print(f\"Test accuracy: {accuracy_score(y_te, te_preds)}\")\n        print(f\"Test precision: {precision_score(y_te, te_preds)}\")\n        print(f\"Test recall: {recall_score(y_te, te_preds)}\")\n        print(f\"Test f1 score: {f1_score(y_te, te_preds)}\")\n        print(f\"Test roc_auc: {roc_auc_score(y_te, te_preds)}\")\n        \n        print('\\nGrid Search Results (you animal):\\n')\n        return pd.DataFrame(estimator.cv_results_)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:55.559926Z","iopub.execute_input":"2021-12-01T03:18:55.560460Z","iopub.status.idle":"2021-12-01T03:18:55.581256Z","shell.execute_reply.started":"2021-12-01T03:18:55.560415Z","shell.execute_reply":"2021-12-01T03:18:55.580303Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":"We'll be using a LogisticRegression model for our baseline as I've had good results with those in past NLP projects and anticipate it performing well with the text data. ","metadata":{}},{"cell_type":"code","source":"logreg = LogisticRegression()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:55.582699Z","iopub.execute_input":"2021-12-01T03:18:55.583145Z","iopub.status.idle":"2021-12-01T03:18:55.597017Z","shell.execute_reply.started":"2021-12-01T03:18:55.583096Z","shell.execute_reply":"2021-12-01T03:18:55.596196Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"evaluate(logreg, X_train_numbers_scaled, X_test_numbers_scaled, \n         y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:55.599643Z","iopub.execute_input":"2021-12-01T03:18:55.600341Z","iopub.status.idle":"2021-12-01T03:18:57.513748Z","shell.execute_reply.started":"2021-12-01T03:18:55.600292Z","shell.execute_reply":"2021-12-01T03:18:57.512931Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"The numbers-only model has an accuracy score of 56%, which with the balanced dataset means that this model only works slightly better then guessing. ","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing\n\nNow that we've establishted the baseline for a model using recipe text it's time to move onwards towards the real modeling. Before this can be done the dataframe needs to be vectorized - I'll be using both CountVectorizer and TfidfVectorizer because I want to see if one of the two has a better performance. ","metadata":{}},{"cell_type":"code","source":"# review what columns we need to vectorize\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:57.514875Z","iopub.execute_input":"2021-12-01T03:18:57.515115Z","iopub.status.idle":"2021-12-01T03:18:58.596151Z","shell.execute_reply.started":"2021-12-01T03:18:57.515086Z","shell.execute_reply":"2021-12-01T03:18:58.595214Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"# creating a dataframe with the text that will be used in the model as well as the target\ndf_strings = df[['description','steps','search_terms','tags', 'target']]\ndf_strings.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:58.597454Z","iopub.execute_input":"2021-12-01T03:18:58.597715Z","iopub.status.idle":"2021-12-01T03:18:58.648714Z","shell.execute_reply.started":"2021-12-01T03:18:58.597665Z","shell.execute_reply":"2021-12-01T03:18:58.647771Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"# use the basic_cleaning function on each non-list column of the dataframe \ndf_strings = basic_cleaning(df_strings, 'description')\ndf_strings = basic_cleaning(df_strings, 'steps')\ndf_strings = basic_cleaning(df_strings, 'tags')\ndf_strings = basic_cleaning(df_strings, 'search_terms')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:18:58.650080Z","iopub.execute_input":"2021-12-01T03:18:58.650292Z","iopub.status.idle":"2021-12-01T03:20:14.582237Z","shell.execute_reply.started":"2021-12-01T03:18:58.650265Z","shell.execute_reply":"2021-12-01T03:20:14.581073Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"# confirming the cleaning process worked as expected\ndf_strings.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:20:14.583794Z","iopub.execute_input":"2021-12-01T03:20:14.584601Z","iopub.status.idle":"2021-12-01T03:20:14.599816Z","shell.execute_reply.started":"2021-12-01T03:20:14.584558Z","shell.execute_reply":"2021-12-01T03:20:14.598852Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"markdown","source":"Note that for search terms and tags removal of the '-' has insered words like 'timetomake' into those categories.  I'm not concerend that they aren't real words because they appear consistently throughout their columns and thus add value to the model.","metadata":{}},{"cell_type":"code","source":"# dropping unnecessary columns\ndf_strings.drop(columns = ['description','steps','search_terms','tags'], inplace=True)\ndf_strings","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:20:14.601368Z","iopub.execute_input":"2021-12-01T03:20:14.601778Z","iopub.status.idle":"2021-12-01T03:20:14.899567Z","shell.execute_reply.started":"2021-12-01T03:20:14.601734Z","shell.execute_reply":"2021-12-01T03:20:14.898509Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"# splitting the data to X and y \ny = df_strings['target']\nX = df_strings.drop('target', axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:20:14.901127Z","iopub.execute_input":"2021-12-01T03:20:14.901425Z","iopub.status.idle":"2021-12-01T03:20:14.949518Z","shell.execute_reply.started":"2021-12-01T03:20:14.901386Z","shell.execute_reply":"2021-12-01T03:20:14.948733Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"# undersampling the data in the same manner used with the baseline model\nrus = RandomUnderSampler(random_state=50)\nX_res, y_res = rus.fit_resample(X, y)\ny_res.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:20:14.950883Z","iopub.execute_input":"2021-12-01T03:20:14.951199Z","iopub.status.idle":"2021-12-01T03:20:15.363921Z","shell.execute_reply.started":"2021-12-01T03:20:14.951165Z","shell.execute_reply":"2021-12-01T03:20:15.363155Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"# splitting the data, including creating a holdout set\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = .3, \n                                                    random_state=50)\nX_test, X_holdout, y_test, y_holdout = train_test_split(X_test, y_test, test_size = .3,\n                                                        random_state=50)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:20:15.365159Z","iopub.execute_input":"2021-12-01T03:20:15.365471Z","iopub.status.idle":"2021-12-01T03:20:15.545256Z","shell.execute_reply.started":"2021-12-01T03:20:15.365440Z","shell.execute_reply":"2021-12-01T03:20:15.544222Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":"To prepare the data for vectorization we need to combine the columns containing text that will be used in the model to meet the input requirements for the vectorizers.","metadata":{}},{"cell_type":"code","source":"X_train['combined'] = X_train['cleaned_description'].str.cat(X_train[['cleaned_steps',\n                                                                      'cleaned_tags',\n                                                                      'cleaned_search_terms']],sep=\" \")\n\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:20:15.549741Z","iopub.execute_input":"2021-12-01T03:20:15.549984Z","iopub.status.idle":"2021-12-01T03:20:16.218890Z","shell.execute_reply.started":"2021-12-01T03:20:15.549955Z","shell.execute_reply":"2021-12-01T03:20:16.218258Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"# repeating this with the test data\nX_test['combined'] = X_test['cleaned_description'].str.cat(X_test[['cleaned_steps',\n                                                                'cleaned_tags',\n                                                                'cleaned_search_terms']],sep=\" \")\nX_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:20:16.220135Z","iopub.execute_input":"2021-12-01T03:20:16.220535Z","iopub.status.idle":"2021-12-01T03:20:16.437249Z","shell.execute_reply.started":"2021-12-01T03:20:16.220487Z","shell.execute_reply":"2021-12-01T03:20:16.436436Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import (\n    BernoulliNB,\n    ComplementNB,\n    MultinomialNB,\n)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:20:16.438890Z","iopub.execute_input":"2021-12-01T03:20:16.439623Z","iopub.status.idle":"2021-12-01T03:20:16.465345Z","shell.execute_reply.started":"2021-12-01T03:20:16.439583Z","shell.execute_reply":"2021-12-01T03:20:16.464459Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"markdown","source":"### CountVectorizer","metadata":{}},{"cell_type":"code","source":"# having the CountVectorizer remove stop words\ncountvect = CountVectorizer(stop_words=stop_words, ngram_range=(1,1))","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:20:16.466595Z","iopub.execute_input":"2021-12-01T03:20:16.466866Z","iopub.status.idle":"2021-12-01T03:20:16.470712Z","shell.execute_reply.started":"2021-12-01T03:20:16.466834Z","shell.execute_reply":"2021-12-01T03:20:16.470119Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"X_train_CV = countvect.fit_transform(X_train.combined)\nX_test_CV = countvect.transform(X_test.combined)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:20:16.471737Z","iopub.execute_input":"2021-12-01T03:20:16.472317Z","iopub.status.idle":"2021-12-01T03:20:44.799281Z","shell.execute_reply.started":"2021-12-01T03:20:16.472280Z","shell.execute_reply":"2021-12-01T03:20:44.798270Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"classifiers = {\n    \"BernoulliNB\": BernoulliNB(),\n    \"ComplementNB\": ComplementNB(),\n    \"MultinomialNB\": MultinomialNB(),\n    \"KNeighborsClassifier\": KNeighborsClassifier(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(max_depth=3), #to keep the initial modeling quick\n    \"RandomForestClassifier\": RandomForestClassifier(max_depth=3),\n    \"LogisticRegression\": LogisticRegression(),\n    \"AdaBoostClassifier\": AdaBoostClassifier(),\n}","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:20:44.800569Z","iopub.execute_input":"2021-12-01T03:20:44.800918Z","iopub.status.idle":"2021-12-01T03:20:44.806931Z","shell.execute_reply.started":"2021-12-01T03:20:44.800882Z","shell.execute_reply":"2021-12-01T03:20:44.806129Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"for name, sklearn_classifier in classifiers.items():\n    classifier = sklearn_classifier\n    print(name)\n    evaluate(classifier, X_train_CV, X_test_CV, y_train, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}